
# -*- coding: utf-8 -*-
"""TextProcessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I6zUYxlLBbu-MqVPXnNSJPBOz2jThEt7
"""

# Install BERT for tf2 module
!pip install bert-for-tf2
# Install sentencepiece library for text cleaning
!pip install sentencepiece

# Commented out IPython magic to ensure Python compatibility.
# Import all necessary libraries
try:
#     %tensorflow_version 2.x
except Exception:
    pass

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers
from tensorflow.keras import callbacks
from tensorflow.keras import optimizers
from tensorflow.keras import utils
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
import bert
import os
import numpy as np
import re
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

!pip install pandas numpy scikit-learn nltk spacy gensim wordcloud

# Import the training and test .csv files
import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive',force_remount=True)

# Path to your CSV file
csv_file_path_train = "/content/drive/My Drive/pro/texts/train_data.csv"
csv_file_path_test = "/content/drive/My Drive/pro/texts/test_data.csv"

colnames=['image_path', 'text', 'food']
train = pd.read_csv(csv_file_path_train, names=colnames, header=None, sep = ',', index_col=['image_path'])
test = pd.read_csv(csv_file_path_test, names=colnames, header=None, sep = ',', index_col=['image_path'])

# Cleaning text function

def preprocess_text(sen):
    # Removing html tags
    sentence = remove_tags(sen)

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    sentence = sentence.lower()

    return sentence

def remove_tags(text):
    return TAG_RE.sub('', text)

TAG_RE = re.compile(r'<[^>]+>')
vec_preprocess_text = np.vectorize(preprocess_text)

nClasses = train.food.nunique()

encoder = LabelEncoder()
processed_train = vec_preprocess_text(train.text.values)
processed_test = vec_preprocess_text(test.text.values)


encoded_labels_train = encoder.fit_transform(train.food.values)
labels_train = utils.to_categorical(encoded_labels_train, nClasses)

encoded_labels_test = encoder.fit_transform(test.food.values)
labels_test = utils.to_categorical(encoded_labels_test, nClasses)

# Import the BERT BASE model from Tensorflow HUB (layer, vocab_file and tokenizer)
BertTokenizer = bert.bert_tokenization.FullTokenizer
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1",trainable=False)
vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = BertTokenizer(vocabulary_file, to_lower_case)

# Preprocessing of texts according to BERT

def get_masks(text, max_length):
    """Mask for padding"""
    tokens = tokenizer.tokenize(text)
    tokens = ["[CLS]"] + tokens + ["[SEP]"]
    length = len(tokens)
    if length > max_length:
        tokens = tokens[:max_length]

    return np.asarray([1]*len(tokens) + [0] * (max_length - len(tokens)))
vec_get_masks = np.vectorize(get_masks, signature = '(),()->(n)')

def get_segments(text, max_length):
    """Segments: 0 for the first sequence, 1 for the second"""
    tokens = tokenizer.tokenize(text)
    tokens = ["[CLS]"] + tokens + ["[SEP]"]
    length = len(tokens)
    if length > max_length:
        tokens = tokens[:max_length]

    segments = []
    current_segment_id = 0
    with_tags = ["[CLS]"] + tokens + ["[SEP]"]
    token_ids = tokenizer.convert_tokens_to_ids(tokens)

    for token in tokens:
        segments.append(current_segment_id)
        if token == "[SEP]":
            current_segment_id = 1
    return np.asarray(segments + [0] * (max_length - len(tokens)))
vec_get_segments = np.vectorize(get_segments, signature = '(),()->(n)')

def get_ids(text, tokenizer, max_length):
    """Token ids from Tokenizer vocab"""
    tokens = tokenizer.tokenize(text)
    tokens = ["[CLS]"] + tokens + ["[SEP]"]
    length = len(tokens)
    if length > max_length:
        tokens = tokens[:max_length]

    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = np.asarray(token_ids + [0] * (max_length-length))
    return input_ids
vec_get_ids = np.vectorize(get_ids, signature = '(),(),()->(n)')


def prepare(text_array, tokenizer, max_length = 128):

    ids = vec_get_ids(text_array,
                      tokenizer,
                      max_length).squeeze()
    masks = vec_get_masks(text_array,
                      max_length).squeeze()
    segments = vec_get_segments(text_array,
                      max_length).squeeze()

    return ids, segments, masks

max_length = 30 # that must be set according to your dataset
ids_train, segments_train, masks_train = prepare(processed_train,tokenizer,max_length)
ids_test, segments_test, masks_test = prepare(processed_test,tokenizer,max_length)

input_word_ids = layers.Input(shape=(max_length,), dtype=tf.int32,
                                       name="input_word_ids")
input_mask = layers.Input(shape=(max_length,), dtype=tf.int32,
                                   name="input_mask")
segment_ids = layers.Input(shape=(max_length,), dtype=tf.int32,
                                    name="segment_ids")
den_out, seq_out = bert_layer([input_word_ids, input_mask, segment_ids])

# Attention layer
from keras import backend as K

class AttentionLayer(layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1), initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)
        at = K.softmax(et)
        at = K.expand_dims(at, axis=-1)
        output = x * at
        return K.sum(output, axis=1)

import tensorflow as tf
from tensorflow.keras import layers, models

class WordAttention(layers.Layer):
    def __init__(self, units):
        super(WordAttention, self).__init__()
        self.W = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, inputs):
        # inputs shape: (batch_size, max_length, embedding_dim)
        score = tf.nn.tanh(self.W(inputs))
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        context_vector = attention_weights * inputs
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

class SentenceAttention(layers.Layer):
    def __init__(self, units):
        super(SentenceAttention, self).__init__()
        self.W = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, inputs):
        # inputs shape: (batch_size, num_sentences, sentence_embedding_dim)
        score = tf.nn.tanh(self.W(inputs))
        attention_weights = tf.nn.softmax(self.V(score), axis=1)
        sentence_vector = attention_weights * inputs
        sentence_vector = tf.reduce_sum(sentence_vector, axis=1)
        return sentence_vector, attention_weights

from tensorflow.keras import layers

# Define input layers
input_word_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name="input_word_ids")
input_mask = layers.Input(shape=(max_length,), dtype=tf.int32, name="input_mask")
segment_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name="segment_ids")

# BERT layer
_, seq_out = bert_layer([input_word_ids, input_mask, segment_ids])

# Sentence-level attention
sentence_context_vector, _ = SentenceAttention(128)(seq_out)

# Reshape sentence context vector to add a sequence dimension
sentence_context_expanded = layers.Reshape((-1, 128))(sentence_context_vector)

# Bidirectional GRU layer after sentence-level attention
bidirectional_gru_out = layers.Bidirectional(layers.GRU(128, return_sequences=True))(sentence_context_expanded)

# Hierarchical attention
document_context_vector, _ = WordAttention(128)(bidirectional_gru_out)

# Reshape the document context vector to make it compatible with the Dense layer
document_context_vector_expanded = layers.Reshape((-1,))(document_context_vector)

# Additional dense layers
dense_out_1 = layers.Dense(256, activation="relu")(document_context_vector_expanded)
dense_dropout_out_1 = layers.Dropout(0.5)(dense_out_1)

# Output layer
output = layers.Dense(nClasses, activation='softmax')(dense_dropout_out_1)

# Define the model
model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)

# Adam optimizer
opt = optimizers.Adam(learning_rate=0.01)
from tensorflow.keras import optimizers

# Compile model
model.compile(loss = 'categorical_crossentropy',
              optimizer = opt,
              metrics = ['accuracy'])

es = callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)
# Setup callbacks, logs and early stopping condition
checkpoint_path = "/content/drive/My Drive/DL_project/textweights/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5"
cp = callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy',save_best_only=True,verbose=1, mode='max')
csv_logger = callbacks.CSVLogger('/content/drive/My Drive/DL_project/textweights/bert_hans_gru.log')
es = callbacks.EarlyStopping(patience = 3, restore_best_weights=True)
# Reduce learning rate if no improvement is observed
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=1, min_lr=0.00001)

from keras.callbacks import ReduceLROnPlateau, EarlyStopping

# Define ReduceLROnPlateau callback
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)

# Define EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with ReduceLROnPlateau and EarlyStopping callbacks
history = model.fit([ids_train, masks_train, segments_train],
                    labels_train,
                    epochs=16,
                    batch_size=30,
                    validation_split=0.3,
                    callbacks=[cp, reduce_lr, early_stopping])

model.evaluate([ids_test, masks_test, segments_test],
               labels_test,
               batch_size = 512)
